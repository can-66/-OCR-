# 📊 DataScience_Pro: 基于OCR与机器学习的岗位薪资预测系统

> **项目状态**: ✅ 已完成 | **版本**: v2.0 | **最后更新**: 2025-12

## 📖 项目简介

本项目是一个端到端的数据科学工程实践，旨在分析国内"数据分析师"岗位的市场需求与薪资构成。
项目突破了 **Boss直聘** 的高难度反爬虫机制（字体加密、后台冻结），实现了从**真实数据采集**、**ETL清洗**、**NLP特征工程**、**机器学习建模**到**交互式Web大屏展示**的全流程闭环。

核心价值在于利用 **Random Forest** 模型量化了学历、城市及特定技能（如 Spark, Python）对薪资的边际贡献。

---

## 🏗️ 技术架构

* **数据采集层**: `Selenium` + `undetected-chromedriver` (绕过检测) + `ddddocr` (OCR识别加密薪资)
* **数据存储层**: `SQLite` (关系型数据库) + `SQLAlchemy`
* **数据处理层**: `Pandas` (清洗) + `Jieba` (NLP分词) + `Re` (正则提取)
* **算法模型层**: `Scikit-learn` (随机森林回归 RandomForestRegressor)
* **应用交互层**: `Streamlit` (Web仪表盘) + `Plotly` (动态可视化)
* **数据导出层**: `Matplotlib` + `Seaborn` (质量验证图表)

---

## 📂 目录结构

确保你的文件目录结构如下，**切勿搞错 `data` 文件夹的位置**：

```text
DataScience_Pro/
├── data/                          # [数据中心]
│   ├── real_job_data.csv          # 爬虫抓取的原始数据 (含OCR结果)
│   ├── job_data.db                # 清洗后的 SQLite 数据库
│   ├── salary_model.pkl           # 训练好的机器学习模型文件
│   ├── final_clean_dataset.csv    # 最终清洗后的数据集 (CSV格式)
│   ├── final_clean_dataset.xlsx   # 最终清洗后的数据集 (Excel格式)
│   └── salary_verification_plot.png # 数据质量验证图表
├── src/                           # [源代码]
│   ├── spider_pro.py             # 爬虫脚本 (含OCR与自动滚动)
│   ├── etl_pipeline.py            # ETL 清洗与 NLP 处理脚本
│   ├── model_train.py             # 模型训练脚本
│   ├── visual_app.py              # Streamlit 可视化启动脚本
│   └── export_analysis.py         # 数据导出与质量验证脚本
├── main.py                        # 后端主程序入口 (串联所有流程)
├── verify_fix.py                  # 数据质量验证脚本
├── READMD.md                      # 项目说明文档
└── requirements.txt               # 项目依赖库
```

---

## 🚀 快速启动指南 (Quick Start)

### 第一步：环境配置

确保安装了 **Python 3.8+**，并在项目根目录下运行：

```bash
pip install -r requirements.txt
```

### 第二步：数据采集 (Crawler) - 可选

如果你已有 `data/real_job_data.csv` 文件，可以跳过此步骤。

运行爬虫脚本，抓取真实数据：

```bash
python src/spider_pro.py
```

⚠️ **操作提示**:
- 浏览器自动打开后，请手动扫码登录 Boss 直聘
- 手动搜索 "数据分析师" 并确保看到职位列表
- 回到命令行窗口，按 **回车 (Enter)** 键启动自动抓取
- 程序将自动滚动、截图识别薪资并保存至 `data/real_job_data.csv`

### 第三步：数据清洗与建模 (Backend Pipeline)

运行主程序，执行完整的ETL清洗、NLP特征提取、模型训练和数据导出：

```bash
python main.py
```

**执行流程**:
1. **ETL清洗**: 薪资解析、城市修复、学历提取、异常值过滤
2. **特征工程**: NLP技能特征提取（Python, SQL, Spark等）
3. **模型训练**: 随机森林回归模型训练，自动过滤学历不限样本
4. **数据导出**: 生成清洗后的数据集和质量验证图表

**成功标志**:
- ✅ 终端显示 "ETL清洗完成"
- ✅ 终端打印出 "薪资影响因子排行榜" (Feature Importance)
- ✅ `data/` 目录下生成 `job_data.db` 和 `salary_model.pkl`
- ✅ 生成 `final_clean_dataset.csv` 和 `salary_verification_plot.png`

### 第四步：启动可视化大屏 (Dashboard)

启动 Streamlit Web 应用：

```bash
streamlit run src/visual_app.py
```

浏览器将自动打开 `http://localhost:8501`，你将看到：

- 📊 **全景市场洞察**: 城市薪资排行、学历分布、技能价值分析
- 🤖 **AI 薪资预测器**: 输入你的背景信息，模型预测市场价值

---

## 🔧 核心功能模块

### 1. ETL数据清洗 (`etl_pipeline.py`)

**核心功能**:
- ✅ 智能薪资解析：处理各种格式（"15-25K"、"2140K"等）
- ✅ 城市信息修复：从原始文本中准确提取城市（倒序扫描策略）
- ✅ 学历经验提取：自动识别学历要求（大专/本科/硕士/博士）和工作经验
- ✅ 异常值过滤：自动过滤异常高薪（>5万/月）和异常低薪（<2000/月）
- ✅ NLP特征工程：提取技能关键词（Python, SQL, Spark等）

**关键修复**:
- 学历不限（degree_value=0）样本在ETL阶段标记，但不参与模型训练
- 异常薪资样本自动过滤，确保数据质量

### 2. 模型训练 (`model_train.py`)

**核心功能**:
- ✅ 特征工程：将学历作为类别特征（One-Hot编码），避免错误的数值关系
- ✅ 数据过滤：自动过滤学历不限样本和异常高薪样本
- ✅ 模型训练：随机森林回归（200棵树，自动并行）
- ✅ 特征重要性分析：输出影响薪资的关键因子

**关键修复**:
- 学历特征改为类别特征，避免模型学习"大专=1 < 本科=2"的错误数值关系
- 只使用明确的学历要求（1=大专, 2=本科, 3=硕士, 4=博士）进行训练

### 3. 数据导出与验证 (`export_analysis.py`)

**核心功能**:
- ✅ 数据导出：生成CSV和Excel格式的清洗后数据集
- ✅ 质量验证：生成学历-薪资分布箱线图，验证数据合理性
- ✅ 统计报告：输出各学历的平均薪资统计

### 4. 可视化应用 (`visual_app.py`)

**核心功能**:
- ✅ **全景市场洞察**:
  - 各城市薪资竞争力排行
  - 学历门槛分布（饼图）
  - 硬技能含金量分析（技能溢价）
- ✅ **AI薪资预测器**:
  - 交互式输入（城市、学历、经验、技能）
  - 实时薪资预测
  - 预测结果展示

---

## ✨ 项目亮点 (Highlights)

### 1. 攻克字体反爬
针对 Boss 直聘的薪资乱码问题（Font Obfuscation），创新性地使用 **DOM 元素截图 + OCR 深度学习模型** 进行像素级识别，还原真实数据。

### 2. 后台保活机制
通过注入 Chrome Flag (`--disable-renderer-backgrounding`)，解决了浏览器后台运行时 JS 停止渲染的问题，实现全自动挂机采集。

### 3. 非结构化文本挖掘
使用 NLP 技术从杂乱的 JD (职位描述) 中提取出 Python, SQL, Hadoop 等硬技能特征，大幅提升了模型预测精度。

### 4. 工程化落地
避免了脚本式编程，采用面向对象 (OOP) 设计，实现了从数据获取到模型部署的完整工程链路。

### 5. 数据质量保障
- 多层级异常值过滤（ETL阶段 + 模型训练阶段）
- 学历不限样本智能处理（标记但不参与训练）
- 自动生成数据质量验证报告

### 6. 特征工程优化
- 学历作为类别特征而非数值特征，避免模型学习错误的数值关系
- 确保预测结果符合常识：本科 > 大专，硕士 > 本科

---

## 📊 数据质量说明

### 数据过滤规则

1. **异常薪资过滤**:
   - 过滤月薪 > 5万元（可能是OCR错误或单位错误）
   - 过滤月薪 < 2000元（可能是日薪或兼职）

2. **学历样本过滤**:
   - 模型训练时自动过滤 `degree_value=0`（学历不限）的样本
   - 只使用明确的学历要求进行训练

3. **数据验证**:
   - 运行 `python verify_fix.py` 可查看数据质量报告
   - 自动生成 `salary_verification_plot.png` 验证图表

---

## 🛠️ 单独运行模块

如果需要单独运行某个模块：

```bash
# 只运行ETL清洗
python src/etl_pipeline.py

# 只训练模型
python src/model_train.py

# 只导出数据
python src/export_analysis.py

# 验证数据质量
python verify_fix.py
```

---

## 📝 注意事项

1. **数据路径**: 确保 `data/` 文件夹在项目根目录下
2. **编码问题**: CSV文件使用 `utf-8-sig` 编码，兼容Excel打开
3. **模型预测**: 预测时确保输入的特征与训练时一致（城市、学历、经验、技能）
4. **学历映射**: 
   - 1 = 大专
   - 2 = 本科
   - 3 = 硕士
   - 4 = 博士
   - 0 = 学历不限（不参与训练）

---

## 🔍 故障排查

### 问题1: 模型预测结果不合理（大专薪资高于本科）

**解决方案**:
1. 重新运行 `python main.py` 确保使用最新代码
2. 检查数据中是否有异常高薪样本：运行 `python verify_fix.py`
3. 确保模型训练时过滤了学历不限样本

### 问题2: 找不到数据库文件

**解决方案**:
- 先运行 `python src/etl_pipeline.py` 生成数据库
- 检查 `data/` 目录是否存在

### 问题3: Streamlit 启动失败

**解决方案**:
- 确保安装了所有依赖：`pip install -r requirements.txt`
- 检查端口8501是否被占用

---

## 📄 许可证

本项目仅供学习研究使用，请遵守相关网站的使用条款。

---

## 👥 贡献

欢迎提交 Issue 和 Pull Request！

---

**最后更新**: 2025-12 | **维护状态**: ✅ 活跃维护中
